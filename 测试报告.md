# 测试环境

**OpenCloudOS 8：**

腾讯云：2cpu 2G，NUMA节点为1，x86架构

本地qemu：8cpu 8G NUMA节点为2, 镜像是OpencloudOS8.8的qcow2格式镜像，x86架构

**OpenCloudOS 9：**

腾讯云：2cpu 2G，NUMA节点为1，x86架构

本地qemu：8cpu 8G NUMA节点为2，镜像是OpencloudOS9.2的Minimal格式镜像，x86架构

**OpenCloudOS Stream 23：**

本地qemu：8cpu 8G NUMA节点为2，镜像是OpencloudOS Stream 23的Minimal格式镜像，x86架构

# cgroup version1

## blkio

主要是限制cgroup对IO的使用，两种控制策略，weight和throttling。

### weight策略

**需要开启的内核配置：**

- CONFIG_BLK_CGROUP

**接口文件：**

- **blkio.bfq.weight：** 读写文件。指定每个cgroup组的权重。这是组在所有设备上的默认权重，除非被blkio.bfq.weight_device覆盖。目前允许的范围是1到1000

- **blkio.bfq.weight_device：** 读写文件。指定每个cgroup中每个设备的权重，覆盖blkio.bfq.weight

- **blkio.bfq.io_service_bytes：** 只读文件。cgroup传输到磁盘/从磁盘传输的字节数

- **blkio.bfq.io_serviced：** 只读文件。cgroup向磁盘发出的 IO（bio）数量。

**测试用例：**

- **cgroup-v1-blkio-bfq-weight：** 测试接口文件blkio.bfq.weight。
- **cgroup-v1-blkio-bfq-weight_device：** 测试接口文件blkio.bfq.weight_device。

### throttling策略

**需要开启内核配置：**

- CONFIG_BLK_CGROUP
- CONFIG_BLK_DEV_THROTTLING

**接口文件：**

- **blkio.throttle.read_bps_device：** 读写文件。设置从设备读取速率的上限。单位是`byets/s`
- **blkio.throttle.write_bps_device：** 读写文件。设置对设备的写速率上限。单位是`byets/s`
- **blkio.throttle.read_iops_device：** 读写文件。设置从设备读取速率的上限。单位是`IO/s`
- **blkio.throttle.write_iops_device：** 读写文件。设置对设备的写速率上限。单位是`IO/s`
- **bklio.throttle.io_service_bytes：** 只读文件。记录自从cgroup创建以来，对于每种操作的已完成的向磁盘传输/从磁盘传输的字节数。
- **blkio.throttle.io_serviced：** 只读文件。记录自从cgroup创建以来，对于每种操作的已完成的I/O操作数。

**测试用例：**

- **cgroup-v1-blkio-throttle-read_bps_device：** 测试接口文件blkio.throttle.read_bps_device
- **cgroup-v1-blkio-throttle-write_bps_device：** 测试接口文件blkio.throttle.write_bps_device
- **cgroup-v1-blkio-throttle-read_iops_device：** 测试接口文件blkio.throttle.read_iops_device
- **cgroup-v1-blkio-throttle-write_iops_device：** 测试接口文件blkio.throttle.write_iops_device

### 层次结构

throttling实现了层次结构支持，但只有root cgroup启用了sane_behavior选项才启用了throttling层次结构。如果没有启用，则实际上会将所有cgroup视为同一级别。目前这是一个开发选项，尚未公开

### 测试结果

**OpenCloudOS 8：**

腾讯云：

![image-20240813154623977](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813154623977.png)

本地qemu：

![image-20240813233107878](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813233107878.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814131958155](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814131958155.png)

本地qemu：

![image-20240814151255325](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814151255325.png)

cgroup-v1-blkio-throttle-write_bps_device和cgroup-v1-blkio-throttle-write_iops_device测试用例测试失败，原因不明，不能正常地限制硬盘写速度

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814181022711](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814181022711.png)

cgroup-v1-blkio-throttle-write_bps_device和cgroup-v1-blkio-throttle-write_iops_device测试用例测试失败，不能正常地限制硬盘写速度

## cpu

主要是限制cgroup对CPU的使用，两种控制策略，权重比例设置和周期使用时间上限。

### CPU使用周期使用时间上限

**接口文件：**

- **cpu.cfs_period_us：** 读写文件。规定CPU的时间周期（微秒），最大值是1s，最小值是1000微妙。
- **cpu.cfs_quota_us：** 读写文件。在单位时间内（即cpu.cfs_period_us设定值）可用的CPU最大时间（单位是微秒）。cpu.cfs_quota_us值可以大于cpu.cfs_period_us的值，例如在一个双CPU的系统内，想要一个cgroup内的进程充分地利用2个CPU，可以设定cpu.cfs_quota_us为200000及cpu.cfs_period_us为100000
- **cpu.stat： ** 只读文件。统计信息,包含：
  - nr_periods：经历了几个cfs_period_us周期
  - nr_throttled：task被限制的次数
  - throtled_time：表示task被限制的总时长

**测试用例：**

- **cgroup-v1-cpu-cfs-001：** 测试cpu.cfs_period_us和cpu.cfs_quota_us接口文件，cfs_quota_us小于cfs_period_us的情况
- **cgroup-v1-cpu-cfs-002：** 测试cpu.cfs_period_us和cpu.cfs_quota_us接口文件，cfs_quota_us是cfs_period_us两倍的情况

### 权重比例设定CPU的分配

**接口文件：**

- **cpu.shares：** 读写文件。CPU比重分配，通过一个整数的数值调节cgroup所占用的CPU时间。

**测试用例： **

- **cgroup-v1-cpu-shares：** 测试cpu.shares接口文件

### 测试结果

**OpenCloudOS 8：**

腾讯云：

![image-20240813154844123](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813154844123.png)
![image-20240813155220037](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813155220037.png)

本地qemu：

![image-20240813233120405](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813233120405.png)
![image-20240813233137341](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813233137341.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814132028781](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814132028781.png)
![image-20240814132044757](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814132044757.png)

本地qemu：

![image-20240814151409352](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814151409352.png)
![image-20240814151416375](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814151416375.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814181059810](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814181059810.png)
![image-20240814181112081](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814181112081.png)

## cpuacct

显示cgroup中任务所使用的CPU资源

**需要开启的内核配置：**

- CONFIG_CGROUP_CPUACCT

**接口文件：**

- **cpuacct.stat：**  只读文件。报告cgroup的所有任务使用的用户和内核CPU时间，单位为USER_HZ
- **cpuacct.usage：** 只读文件。记录该cgroup中所有进程的CPU使用总时间，以纳秒为单位
- **cpuacct.usage_percpu：** 只读文件。记录该cgroup中所有进程在每个CPU上的使用时间，以纳秒为单位
- **cpuacct.usage_user：** 只读文件。报告一个cgroup中所有任务使用用户态CPU的总时间，以纳秒为单位
- **cpuacct.usage_percpu_user：** 只读文件。报告一个cgroup中所有任务在每个CPU上使用用户态CPU的时间，以纳秒为单位
- **cpuacct.usage_sys：** 只读文件。报告一个cgroup中所有任务在CPU上使用内核态CPU的时间，以纳秒为单位
- **cpuacct.usage_percpu_sys：** 只读文件。报告一个cgroup中所有任务在每个CPU上使用内核态CPU的时间
- **cpuacct.usage_all：** 只读文件。详细输出文件cpuacct.usage_percpu_user和cpuacct.usage_percpu_sys的内容

**测试用例：**

- **cgroup-v1-cpuacct-stat：** 测试cpuacct.stat接口文件
- **cgroup-v1-cpuacct-usage：** 测试cpuacct.usage接口文件
- **cgroup-v1-cpuacct-usage_percpu：** 测试cpuacct.usage_percpu接口文件
- **cgroup-v1-cpuacct-usage_user：**  测试cpuacct.usage_user接口文件
- **cgroup-v1-cpuacct-usage_percpu_user：** 测试cpuacct.usage_percpu_user接口文件
- **cgroup-v1-cpuacct-usage_sys：** 测试cpuacct.usage_sys接口文件
- **cgroup-v1-cpuacct-usage_percpu_sys：** 测试cpuacct.usage_percpu_sys接口文件
- **cgroup-v1-cpuacct-usage_all：** 测试cpuacct.usage_all接口文

**测试结果：**

**OpenCloudOS 8：**

腾讯云：

![image-20240813154921611](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813154921611.png)

cgroup-v1-cpuacct-usage_all、cgroup-v1-cpuacct-usage_percpu_sys和cgroup-v1-cpuacct-usage_sys测试用例失败，原因是这些接口文件不能正确统计cpu内核态使用时间

本地qemu：

![image-20240813233232772](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813233232772.png)

cgroup-v1-cpuacct-usage_all、cgroup-v1-cpuacct-usage_percpu_sys和cgroup-v1-cpuacct-usage_sys测试用例失败，原因是这些接口文件不能正确统计cpu内核态使用时间

**OpenCloudOS 9：**

腾讯云：

![image-20240814132114591](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814132114591.png)

本地qemu：

![image-20240814151440881](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814151440881.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814181155186](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814181155186.png)

## cpuset

为cgroup分配指定的CPU和内存节点

以下规则使用每个cgroup：

- 它的 CPU 和内存节点必须是其父级的子集。
- 除非其父级被标记为exclusive，否则它不能被标记为exclusive。
- 如果其 CPU 或内存是独占的，则它们可能不会与任何兄弟cgroup重叠
- 必须指定当前cgroup的cpuset.mems和cpuset.cpus，才能将进程移入当前cgroup

### cpu相关

**接口文件：**

- **cpuset.cpus：** 读写文件。允许cgroup中的进程使用的CPU列表。
- **cpuset.effective_cpus：** 只读文件。显示了当前cgroup中实际可用于分配给进程的CPU核心列表，它考虑了上级cgroup的限制和当前cgroup的cpuset.cpus配置，从而提供了一个最终的、实际可用的CPU核心集合
- **cpuset.cpu_exclusive：** 读写文件。cgroup是否独占cpuset.cpus中分配的cpu。默认值为0，表示共享；1表示独占。如果设置为1，其他cgroup内的cpuset.cpus值不能包含有该cpuset.cpus内的值
- **cpuset.sched_load_balance：** 读写文件。cgroup的cpu压力是否会被平均到cpuset中的多个cpu上。默认值1，启用负载均衡；0表示禁用。注意：如果任意上级cgroup中启用负载均衡，则在cgroup中设定这个标签没有任何效果，因为已经在较高一级cgroup中处理了负载均衡。因此，要在cgroup中禁用负载平衡，还要在每个上级cgroup中禁用负载平衡。

**测试用例：**

- **cgroup-v1-cpuset-cpus：** 测试cpuset.cpus接口文件
- **cgroup-v1-cpuset-cpu_exclusive：** 测试cpu_exclusive接口文件
- **cgroup-v1-cpuset-sched_load_balance：** 测试cpuset.sched_load_balance接口文件

### memory相关

**接口文件：**

- **cpuset.mems：** 读写文件。允许cgroup中的进程使用的内存节点列表。
- **cpuset.effective_mems：** 只读文件。显示当前cgroup中实际可用于分配给进程的内存节点列表，同样考虑了上级cgroup的限制和当前cgrouop的cpuset.mems配置
- **cpuset.mem_exclusive：** 读写文件。是否独占memory。默认值0表示共享，1表示独占。如果设置为1，其他cgroup内的cpuset.mems值不能包含有该cpuset.mems内的值
- **cpuset.memory_migrate： * 读写文件。用来指定当cpuset.mems中的值更改时是否应该讲内存中的页迁移到新节点。默认情况下是0表示禁止内存迁移，且页就保留在原来分配的节点中，即使在cpuset.mems中现已不再指定这个节点。如果设置为1，则该系统会将页迁移到由cpuset.mems指定的新参数中的内存节点中。
- **cpuset.memory_spread_page：** 读写文件。如果被设置了，将该cpuset中进程上下文是申请的page cache平均分布到cpuset中的各个节点中。默认值是0表示不启用。设置为1表示启用

**测试用例：**

- **cgroup-v1-cpuset-mems：** 测试cpuset.mems接口文件
- **cgroup-v1-cpuset-mem_exclusive：** 测试cpuset.mem_exclusive接口文件
- **cgroup-v1-cpuset-memory_migrate-001：** 测试memory_migrate接口文件, 测试启用内存迁移
- **cgroup-v1-cpuset-memory_migrate-002：** 测试memory_migrate接口文件, 测试不启用内存迁移
- **cgroup-v1-cpuset-memory_spread_page-001：** 测试cpuset.memory_spread_page接口文件，测试不将page cache平均分布道各个节点中
- **cgroup-v1-cpuset-memory_spread_page-002：** 测试cpuset.memory_spread_page接口文件，测试将page cache平均分布道各个节点中

### 测试结果

**OpenCloudOS 8：**

腾讯云：

![image-20240813155138963](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813155138963.png)

部分测试用例需要系统有两个numa节点及以上，所以跳过测试

本地qemu：

![image-20240814102516235](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814102516235.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814132156950](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814132156950.png)

部分测试用例需要系统有两个numa节点及以上，所以跳过测试

本地qemu：

![image-20240814151456351](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814151456351.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814181927733](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814181927733.png)

cgroup-v1-cpuset-cpu_exclusive测试失败，cpuset.cpu_exclusive无法发挥正常作用，无法独占cpu

## devices

来跟踪和执行对设备文件的打开和关闭限制。设备cgroup为每个cgroup关联一个设备接入白名单，白名单有四个字段：

- type：a(all), c(char), b(block)
- 主设备号
- 副设备号
- 访问权限(r ,w, m(mknod))

**需要开启的内核配置：**

- CONFIG_CGROUP_DEVICE

**层次结构：** devices控制器通过确保子cgroup永远不会拥有比其父cgroup更多的访问权限来维护层次结构。每次将条目写入cgroup的devices.deny文件时，它的所有子cgroup都会从白名单中删除该条目，并且所有本地设置的白名单条目都会被重新评估。如果某个本地设置的白名单条目提供了比父cgroup更多的访问权限，它将从白名单中删除

**接口文件：**

- **devices.allow：** 读写文件。允许访问的设备。文件包含4个字段：type（设备类型），major（主设备号），minor（次设备号），access（访问方式）
- **devices.deny：** 读写文件。禁止访问的设备，格式如devices.allow
- **devices.list：** 只读文件。显示目前允许被访问的设备列表

**测试用例：**

- **cgroup-v1-devices-001：** 测试devices.allow接口文件和devices.deny接口文件
- **cgroup-v1-devices-002：** 测试deivces的层级结构

**测试结果：**

**OpenCloudOS 8：**

腾讯云：

![image-20240813155239339](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813155239339.png)

本地qemu：

![image-20240814102535908](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814102535908.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814132225566](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814132225566.png)

本地qemu：

![image-20240814151522526](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814151522526.png)

**OpenCloudOS Stream 23： **

本地qemu：

![image-20240814182020730](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814182020730.png)

## freezer

可以启动和停止cgroup中的task

**需要开启的内核配置：**

- CONFIG_CGROUP_FREEZER

**注意事项：**

- 挂起进程时，会连同子进程一同挂起
- 不能将进程移动到处于FROZEN状态的cgroup中
- 只有FROZEN状态和THAWED状态可以被写进freezer.state中，FREEZING状态则不能，只能读取它
- root cgroup是不可冻结的

**接口文件：**

- **freezer.state： ** 读写文件
  - cgroup的三种状态
    - FROZEN：挂起该cgroup中的任务
    - FREEZING：该系统正在挂起该cgroup中的任务
    - THAWED：已经恢复该cgroup中的任务
  - 当被读取时，返回该group的有效状态（也就是上面三种状态）。这是自己和父cgroup状态的结合。如果有FREEZING，则cgroup的状态为FREEZING
  - 当属于cgroup及其子cgroup所有任务被冻结时，cgroup转换为FROZEN。注意，当一个新任务被添加到cgroup或它的子cgroup中，cgroup会从FROZEN状态恢复到FREZING状态，直到新任务被冻结
  - 写入时，设置cgroup的self-state。允许两个值：FROZEN和THAWED。如果写了FROZEN，则cgroup和它的所有子cgroup一起进入FROZEN状态
  - 如果写入THAWED，则cgroup的self-state更改为THAWED。注意，如果父cgroup状态仍处于FROZEN，则有效状态可能不会被更改为THAWED。如果cgroup的有效状态变为THAWED，则由于cgroup而冻结的所有子cgroup将离开FROZEN状态
- **freezer.self_freezing： ** 只读文件。显示self-state。如果是0，则为THAWED状态；否则为1。如果最后一次写入了FORZEN，这个值为1
- **freezer.parent_freezing： ** 只读文件。显示parent-state。如果cgroup的父cgroup不是FROZEN，则为0；否则为1

**测试用例： **

- **cgroup-v1-freezer-001：** 测试freezer的正常使用，即停止task和再次启动task
- **cgroup-v1-freezer-002：** 测试freezer的层次结构

**测试结果：**

**OpenCloud 8：**

腾讯云： 

![image-20240813155303515](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813155303515.png)

本地qemu：

![image-20240814102549259](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814102549259.png)

**OpenCloud 9：**

腾讯云： 

![image-20240814132303942](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814132303942.png)

本地qemu：

![image-20240814151819447](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814151819447.png)

**OpenCloud Stream 23：**

本地qemu：

![image-20240814182034409](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814182034409.png)

## hugetlb

允许限制cgroup的hugepage使用量和hugepage预留。

**需要的内核配置：**

- CONFIG_CGROUP_HUGETLB
- CONFIG_HUGETLB_PAGE
- CONFIG_HUGETLBFS

### hugepage使用量

**接口文件：**

- **hugetlb.2MB.limit_in_bytes：** 读写文件。设置hugepage大小为2MB的hugepage使用量限制
- **hugetlb.1GB.limit_in_bytes：** 读写文件。设置hugepage大小为1GB的hugepage使用量限制
- **hugetlb.2MB.max_usage_in_bytes：** 只读文件。显示hugepage大小为2MB最大hugepages使用记录
- **hugetlb.1GB.max_usage_in_bytes：** 只读文件。显示hugepage大小为1GB最大hugepages使用记录
- **hugetlb.2MB.usage_in_bytes：** 只读文件。显示hugepages大小为2MB的hugepage的当前使用量
- **hugetlb.1GB.usage_in_bytes：** 只读文件。显示hugepages大小为1GB的hugepage的当前使用量
- **hugetlb.2MB.failcnt：** 只读文件。显示hugepage大小为2MB由于hugepage使用限制导致的分配失败
- **hugetlb.1GB.failcnt：** 只读文件。显示hugepage大小为1GB由于hugepage使用限制导致的分配失败

**测试用例：**

- **cgropu-v1-hugetlb-limit_in_bytes-001：** 测试hugetlb.2MB.limit_in_bytes接口文件，超过限制
- **cgroup-v1-hugetlb-limit_in_bytes-002：** 测试hugetlb.2MB.limit_in_bytes接口文件, 不超过限制
- **cgroup-v1-hugetlb-limit_in_bytes-003：** 测试hugetlb.1GB.limit_in_bytes接口文件，超过限制
- **cgroup-v1-hugetlb-limit_in_bytes-004：** 测试hugetlb.1GB.limit_in_bytes接口文件, 不超过限制

### hugepage预留

**接口文件：**

- **hugetlb.2MB.rsvd.limit_in_bytes：** 读写文件。设置hugepage大小为2MB的hugepage预留限制
- **hugetlb.1GB.rsvd.limit_in_bytes：** 读写文件。设置hugepage大小为1GB的hugepage预留限制
- **hugetlb.2MB.rsvd.max_usage_in_bytes：** 只读文件。显示hugepage大小为2MB最大hugepages预留记录
- **hugetlb.1GB.rsvd.max_usage_in_bytes：** 只读文件。显示hugepage大小为1GB最大hugepages预留记录
- **hugetlb.2MB.rsvd.usage_in_bytes：** 只读文件。显示hugepages大小为2MB的hugepage的当前预留量
- **hugetlb.1GB.rsvd.usage_in_bytes：** 只读文件。显示hugepages大小为1GB的hugepage的当前预留量
- **hugetlb.2MB.rsvd.failcnt：** 只读文件。显示hugepage大小为2MB由于hugepage预留限制导致的分配失败
- **hugetlb.1GB.rsvd.failcnt：** 只读文件。显示hugepage大小为1GB由于hugepage预留限制导致的分配失败

**测试用例：**

- **cgropu-v1-hugetlb-rsvd-limit_in_bytes-001：** 测试hugetlb.2MB.rsvd.limit_in_bytes接口文件，超过限制
- **cgroup-v1-hugetlb-rsvd-limit_in_bytes-002：** 测试hugetlb.2MB.rsvd.limit_in_bytes接口文件, 不超过限制
- **cgroup-v1-hugetlb-rsvd-limit_in_bytes-003：** 测试hugetlb.1GB.rsvd.limit_in_bytes接口文件，超过限制
- **cgroup-v1-hugetlb-rsvd-limit_in_bytes-004：** 测试hugetlb.1GB.rsvd.limit_in_bytes接口文件, 不超过限制

### 测试结果

**OpenCloudOS 8：**

腾讯云：

![image-20240813161618198](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813161618198.png)

跳过的测试是需要在hugepage大小为1G的情况下测试，而rsvd测试失败是因为系统hugetlb.\<hugepage_size\>.rsvd.limit_in_bytes接口文件。

本地qemu：

![image-20240814102626444](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814102626444.png)

rsvd测试失败是因为系统hugetlb.\<hugepage_size\>.rsvd.limit_in_bytes接口文件。

修改hugepage大小为1G后测试：（系统内存至少有8G）

![image-20240814105728307](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814105728307.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814132333013](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814132333013.png)

本地qemu：

![image-20240814151841682](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814151841682.png)

修改hugepage大小为1G后测试：

![image-20240814153748435](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814153748435.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814182050676](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814182050676.png)

修改hugepage大小为1G后测试：

![image-20240814183418474](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814183418474.png)

## memory 

主要是限制cgroup所能使用的内存上限

**需要的内核配置：**

- CONFIG_MEMCG 
- CONFIG_SWAP

**接口文件：**

- **memory.usage_in_bytes：** 只读文件。显示当前内存使用情况，单位为字节
- **memory.memsw.usage_in_bytes：** 只读文件。显示当前内存+交换的使用情况，单位为字节
- **memory.max_uasge_in_bytes：** 只读文件。显示记录的最大内存使用量，单位为字节
- **memory.memsw.max_usage_in_bytes：** 只读文件。显示记录的最大内存+交换使用量，单位为字节
- **memory.failcnt：** 只读文件。显示内存达到memory.limit_in_bytes设定的限制值的次数
- **memory.memsw.failcnt：** 只读文件。显示内存+swap空间限制达到在memory.memsw.limit_in_bytes设定的值的次数
- **memory.limit_in_bytes：** 读写文件。设定最大的内存使用量（包括文件缓存），可以加单位(k/K, m/M, g/G)不加单位默认为bytes。不能限制root cgroup。写入-1删除现有的限制
- **memory.soft_limit_in_bytes：** 读写文件。和memory.limit_in_bytes的差异是，这个限制并不会阻止进程使用超过限额的内存，只是在系统内存不足时，会优先回收超过限额的进程占用的内存，使之向限定值靠拢。该值应小于memory.limit_in_bytes设定值
- **memory.memsw.limit_in_bytes：** 读写文件。设定最大的内存+swap的用量之和。可以加单位(k/K, m/M, g/G)不加单位默认为bytes。不能限制根cgroup。写入-1删除现有的限制
- **memory.force_empty：** 只写文件。当写入0时，清空该group的所有内存页。该选项只有在当前group没有tasks才可以使用。删除cgroup前请使用memory.force_empty以避免将不再使用的页面缓存移动到它的上级cgroup中
- **memory.swappiness：** 读写文件。将内核倾向设定为换出这个cgroup中任务所使用的进程内存，而不是从页缓冲中再生页面。这也是在/proc/sys/vm/swappiness中设定的使用同一方法为整个系统设定的内核倾向。默认值为60，低于这个值会降低内核换出进程内存的倾向，将其设定为0则完成不会为cgroup中的任务换出进程内存。高于这个值将提高内核换出进程内存的倾向，等于100时内核将开始换出作为这个cgroup中进程的地址空间的一部分页面。不能更改一下群组的swappniess：根cgroup，它使用在/proc/sys/vm/swappiness中设定的swappiness。
- **cgroup.event_control：** 只写文件。event_fd()的接口
- **memory.pressure_level：** 读写文件。设置内存压力通知。压力级别通知用于监控内存分配成本；基于压力，应用程序可以实现不同的内存资源管理策略。
- **memory.oom_control：** 读写文件。当进程出现Out of Memory时，是否进行kill操作。默认值0，表示kill；设置为1时，进程将进入睡眠状态，等待内存充足时被唤醒
  - oom_kill_disable：当设置为0时，表示在这个cgroup内存不足时，OOM killer是启用的，可以杀死大量占用内存的进程来释放内存；当值为1时，表示禁用OOM killer，即使内存不足也不会自动杀死进程
  - under_oom：当值为0时，表示当前没有OOM条件发生在这个cgroup中；当值为1时，表示这个cgroup目前正处于oom状态，即系统已经检测到内存不足的情况，并可能正在尝试杀死一些进程来释放内存
  - oom_kill：这个字段显示了由于OOM条件而被杀死的进程数量。每次OOM killer杀死cgroup中的一个进程时，这个计数器就会增加

**测试用例：**

- **cgroup-v1-memory-limit_in_bytes：** 测试memory.limit_in_bytes接口文件，使用了memory.max_uasge_in_bytes和memory.failcnt接口文件
- **cgroup-v1-memory-soft_limit_in_bytes：** 测试memory.soft_limit_in_bytes控制文件
- **cgroup-v1-memory-memsw-limit_in_byte：** 测试memory.memsw.limit_in_bytes接口文件，使用了memory.memsw.max_uasge_in_bytes和memory.memsw.failcnt接口文件
- **cgroup-v1-memory-swappiness-001：**  测试memory.swappiness控制文件，当swappiness为0时，内存不会被交换到swap空间
- **cgroup-v1-memory-swappiness-002：** 测试memory.swappiness控制文件，当swappiness为100时，内存会被交换到swap空间
- **cgroup-v1-memory-pressure_level：** 测试memory.pressure_level控制文件，设置内存压力通知
- **cgroup-v1-memory-oom_control-001：** 测试memory.oom_control接口文件，默认是启用状态
- **cgroup-v1-memory-oom_control-002：** 测试memory.oom_control接口文件，禁用oom_control

**测试结果：**

**OpenCloudOS 8：**

腾讯云：

![image-20240814160924162](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814160924162.png)

cgroup-v1-memory-swappiness-001/002测试跳过的原因是系统没有设置swapfile

本地qemu：

![image-20240814103735716](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814103735716.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814162842038](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814162842038.png)

cgroup-v1-memory-swappiness-001/002测试跳过的原因是系统没有设置swapfile

本地qemu：

![image-20240814153132555](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814153132555.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814182522372](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814182522372.png)

## net_cls

使用等级标识符(classid)标记网络数据包，可允许linux流量控制程

序(traffic controller, tc)识别从具体cgroup中生成的数据包，为来自不同cgroup的报文分配不同的优先级

**需要开启的内核配置：**

- CONFIG_CGROUP_NET_CLASSID

**接口文件：**

- **net_cls.classid：** 读写文件。包含一个说明流量控制句柄的十六进制的值，初始值为0。。这些句柄的格式为0xAAAABBBB，其中AAAA是十六进制主设备号，BBBB是十六进制副设备号。

**测试用例：**

- **cgroup-v1-net_cls-001：** 测试设置带宽限制的情况
- **cgroup-v1-net_cls-002：** 测试不设置带宽限制的情况

**测试结果：**

**OpenCloudOS 8：**

腾讯云：

![image-20240813164446290](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813164446290.png)

本地qemu：

![image-20240814102723332](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814102723332.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814133153525](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814133153525.png)

本地qemu：

![image-20240814152225938](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814152225938.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814182535354](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814182535354.png)

## net_prio

允许设置各种应用程序产生的网络流量的优先级

**需要开启的内核配置：**

- CONFIG_CGROUP_NET_PRIO

**接口文件：**

- **net_prio.ifpriomap：** 读写文件。该文件包含分配给来自该组进程的流量的优先级映射。这些流量从不同的接口流出系统。

**测试用例：**

- **cgroup-v1-net_prio：** 测试net_prio.ifpriomap是否能成功设置

**测试结果：**

**OpenCloudOS 8：**

腾讯云：

![image-20240813164459881](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813164459881.png)

本地qemu：

![image-20240814102741707](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814102741707.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814133210805](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814133210805.png)

本地qemu：

![image-20240814152237975](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814152237975.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814182547603](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814182547603.png)

## perf_event

该层级中的所有cgroup可以使用perf工具对这些进程和线程监控，没有可供调节的参数

**测试用例：**

- **cgroup-v1-perf_event：** 测试使用perf监控cgroup

**测试结果：**

**OpenCloudOS 8：**

腾讯云：

![image-20240813164512233](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813164512233.png)

本地qemu：

![image-20240814102757182](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814102757182.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814133229822](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814133229822.png)

本地qemu：

![image-20240814152247856](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814152247856.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814182606386](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814182606386.png)

## pids

允许cgroup在达到一定限制后阻止任何新任务被fork()或clone()

**需要开启的内核配置：**

- CONFIG_CGROUP_PIDS

**接口文件：**

- **pids.max：** 读写文件。设置最大任务数，不能在root cgroup中使用。默认值是max
- **pids.current：** 只读文件。目前cgroup运行的任务数，pids.current有可鞥大于pids.max，通过设置pids.max小于pids.current实现，但是不可能通过fork和clone实现
- **pids.events：** 只读文件。包含事件计数器：
  - max： 由于自身或父cgroup达到限制，cgroup 中 fork 失败的次数

**测试用例： **

- **cgroup-v1-pids-001：** 测试pids.max控制文件, 进程数超过限制
- **cgroup-v1-pids-002：** 测试pids.max控制文件, 进程数不超过限制
- **cgroup-v1-pids-003：** 测试层次结构

**测试结果：**

**OpenCloudOS 8：**

腾讯云：

![image-20240813164526945](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813164526945.png)

本地qemu：

![image-20240814141530725](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814141530725.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814133341911](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814133341911.png)

本地qemu：

![image-20240814152259054](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814152259054.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814182618283](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814182618283.png)

# cgroup version2

## 核心接口文件

### cgroup.type

存在于非root cgroup上的读写单值文件

读取时，表示该cgroup当前的类型，可以是下列值之一：

- domain：正常有效的域cgroup
- domain threaded：作为线程子树根的线程域cgroup
- domain invalid：处于无效状态的cgroup，无法填充或启用控制器，可能允许其成为线程cgroup
- threaded：线程cgroup，是线程子树的成员

通过将threaded写入此文件，可以将cgroup转变为线程cgroup

### cgroup.controllers

所有cgroup上都存在的只读空格分隔值文件

它显示了cgroup可用的所有控制器的空格分隔值列表。控制器没有排序

### cgroup.subtree_control

所有cgroup上都存在的可读写空格分隔值文件。初始为空

读取时，它会显示以空格分割的控制器列表，这些控制器被启用来控制从cgroup到其子组的资源分配

可以写入以空格分隔的带有+或-前缀的控制器列表来启用或禁用控制器。带有+前缀的控制器名称将启用该控制器，带有-前缀的控制器名称将禁用该控制器。如果控制器在列表中出现多次，则最后一个有效。当指定多个启用和禁用操作时，要么全部成功，要么全部失败

### cgroup.event

存在于非root cgroup上的只读平键文件。定义了以下条目。除非另有说明，否则此文件中的值更改会生成文件修改事件

- populated：如果cgroup或其他后代包含任何活动进程，则为1；否则为0
- frozen：如果cgroup被冻结，则为1；否则为0

### cgroup.max.descendants

读写单值文件。默认值为"max"

允许的最大后代cgroup数量。如果实际后代数量相等或更大，则在层次结构中创建新cgroup的尝试将失败

### cgroup.max.deep

读写单值文件。默认值为max

当前cgroup下允许的最大下降深度。如果实际下降深度等于或大于该值，则尝试创建新的子cgroup将失败

### cgroup.freeze

存在于非root cgroup中的读写单值文件。允许的值为0和1。默认值为0

将1写入文件会导致cgroup及其所有后代cgroup被冻结。这意味着所有附属进程都将停止，直到cgroup明确解冻后才会运行。cgroup的冻结可能需要一些时间；此操作完成后，cgroup.events控制文件中的frozen值将更新为1，并发出相应的通知

cgroup可以通过其自身设置或任何祖先cgroup的设置进行解冻。如果任何父cgroup被解冻，则cgroup将保持冻结状态

冻结cgroup中的进程可以通过致命信号终止。它们还可以进入和离开冻结cgroup；要么通过用户的明确移动，要么冻结cgroup与fork()竞争。如果将进程移动到冻结cgroup，它将停止，如果将进程移出冻结cgroup，它将变为运行状态

cgroup的冻结状态不会影响任何cgroup树操作；可以删除冻结的cgroup，也可以创建新的子cgroup

### cgroup.kill

存在于非root cgroup中的只写单值文件。唯一允许的值是1

将1写入文件会导致cgroup及其所有后代cgroup被终止。这意味着受影响的cgroup树种的所有进程都将通过SIGKILL终止

### cgroup.pressure

允许值为0和1的读写单值文件。默认值为1

向文件写入0将禁用cgroup PSI记。向文件写入1将重新启用cgroup PSI。

此控制属性不是分层的，因此在cgroup中禁用或启用PSI不会影响后代中的PSI，并且不需要通过来自root cgroup传递启用

### 测试用例

- **cgroup-v2-type-001：** 测试cgroup.type接口文件，当子cgroup设置为threaded时，父cgroup自动设置为domain threaded
- **cgroup-v2-type-002：**  测试cgroup.type接口文件, 测试层次结构约束
- **cgroup-v2-subtree_control-001：** 测试cgroup.subtree_control接口文件，在启用了subtree_control的cgroup中，不能够在domain cgroup中加入进程，
- **cgroup-v2-subtree_control-002：** 测试cgroup.subtree_control接口文件，子cgroup的subtree_control只能包含父cgroup的subtree_control
- **cgroup-v2-subtree_control-003：** 测试cgroup.subtree_control接口文件, 不能禁用父cgroup的subtree_control因为子cgroup的subtree_control包含父cgroup的subtree_control
- **cgroup-v2-subtree_control-004：** 测试cgroup.subtree_control接口文件, 在threaded cgroup能够加入线程在启用了subtree_control的cgroup中
- **cgroup-v2-populated：** 测试cgroup.events中的populated字段
- **cgroup-v2-max-descendants：** 测试cgroup.max.descendants接口文件
- **cgroup-v2-max-deep：** 测试cgroup.max.deep接口文件
- **cgroup-v2-freeze-001：** 测试cgroup.freeze接口文件
- **cgroup-v2-freeze-002：** 测试cgroup.freeze接口文件, 测试层次结构
- **cgroup-v2-freeze-003：** 测试cgroup.freeze接口文件, 测试层次结构，测试如果将进程移动到冻结cgroup，它将停止
- **cgroup-v2-freeze-004：** 测试cgroup.freeze接口文件, 测试层次结构，测试cgroup的冻结状态不会影响任何cgroup树操作
- **cgroup-v2-freeze-005：** 测试cgroup.freeze接口文件, 测试迁移的情况
- **cgroup-v2-kill-001：** 测试cgroup.kill接口文件
- **cgroup-v2-kill-002：** 测试cgroup.kill接口文件, 测试层次结构

### 测试结果

**OpenCloudOS 8：**

腾讯云：

![image-20240813175902750](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813175902750.png)
![image-20240813180012630](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813180012630.png)
![image-20240813180113229](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813180113229.png)

cgroup-v2-kill-001/002测试用例失败原因是不存在cgroup.kill这个接口文件

本地qemu：

![image-20240814111432108](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814111432108.png)
![image-20240814111439428](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814111439428.png)
![image-20240814111452398](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814111452398.png)
![image-20240814111510069](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814111510069.png)

cgroup-v2-kill-001/002测试用例失败原因是不存在cgroup.kill这个接口文件

**OpenCloudOS 9：**

腾讯云：

![image-20240814134324218](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814134324218.png)
![image-20240814134339407](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814134339407.png)
![image-20240814134349799](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814134349799.png)

本地qemu：

![image-20240814154750773](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814154750773.png)
![image-20240814222204119](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814222204119.png)
![image-20240814222221561](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814222221561.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814184416348](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814184416348.png)
![image-20240814225750443](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814225750443.png)
![image-20240814225759870](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814225759870.png)

## 控制器接口文件

### CPU

CPU控制器调节CPU周期的分配，为普通调度策略实现权重和绝对带宽限制模型。

所有接口文件的时间长度均以微妙为单位

#### cpu.stat

只读平键文件。无论控制器是否启用，此文件都存在

它始终报告以下三个统计数据：

- usage_usec：CPU使用时间
- user_usec：CPU用户态时间
- system_usec：CPU内核态时间

当控制器启用时，还有以下五个统计数据：

- nr_periods：调度周期的总数
- nr_throttled：进程被限制(throttled)的次数
- throttled_usec：进程被限制的总时间
- nr_bursts：发生的burst事件的次数。burst事件是指在短时间内CPU使用率突然增加的情况
- burst_usec：burst事件的总时间

#### cpu.weight

存在于非root cgroup上的读写单值文件。默认值为100

对于非空闲组(cpu.idle = 0)，权重在范围内[1, 10000]

如果cgroup已配置为SCHED_IDLE(cpu.idle = 1)，则权重将显示为0

#### cpu.weight.nice

存在于非root cgroup上的读写单值文件。默认值为0

nice值的范围是[-20, 19]

此接口文件是cpu.weight的替代接口，允许使用 nice(2)使用的相同值读取和设置权重。由于nice值的范围较小，粒度较粗，因此读取的值是当前权重的最接近近似值

nice(2)是一个系统调用，用于设置进程的优先级，优先级决定了进程在系统资源（特别是CPU时间）分配中的优先顺序。nice值的范围通常是从-20到19，值越小表示越高优先级

#### cpu.max

存在于非root cgroup上的读写双值文件。默认值为" max 100000"

最大带宽限制。其格式如下：

```bash
$MAX $PERIOD
```

表示该组在每个\$PERIOD期间最多可以消费\$MAX。\$MAX中的max表示没有限制，如果只写一个数字，则更新\$MAX

#### cpu.max.burst

存在于非root cgroup上的读写单值文件。默认值为0

burst范围在[0, $MAX]内，这里的MAS是cpu.max中定义的

#### cpu.pressure

读写嵌套键文件。显示CPU的压力失速信息

#### cpu.idle

存在于非root cgroup上的读写单值文件。默认值为0

这是cgroup中每个任务的SCHD_IDLE调度策略的模拟。将此值设置为1将使cgroup的调度策略成为SCHED_IDLE。cgroup中的线程将保留其自己的相对优先级，但cgroup本身将被视为相对于其他同类而言优先级非常低

#### 测试用例

- **cgroup-v2-cpu-stat：** 测试cpu.stat接口文件
- **cgroup-v2-cpu-weight-001：** 测试cpu.weight接口文件
- **cgroup-v2-cpu-weight-002：** 测试cpu.weight接口文件，测试层次结构
- **cgroup-v2-cpu-weight-nice-001：** 测试cpu.weight.nice接口文件
- **cgroup-v2-cpu-weight-nice-002：** 测试cpu.weight.nice接口文件, 测试层次结构
- **cgroup-v2-cpu-max：** 测试cpu.max接口文件
- **cgroup-v2-cpu-max-burst-001：** 测试cpu.max.burst接口文件，设置限制，并设置burst时间
- **cgroup-v2-cpu-max-burst-002：** 测试cpu.max.burst接口文件，设置限制，不设置burst时间
- **cgroup-v2-cpu-pressure：** 测试cpu.pressure接口文件，设置CPU压力通知
- **cgroup-v2-cpu-idle-001：** 测试cpu.idle接口文件，设置SCHD_IDLE调度策略
- **cgroup-v2-cpu-idle-002：** 测试cpu.idle接口文件，不设置SCHD_IDLE调度策略

#### 测试结果

**OpenCloudOS 8：**

腾讯云：

![image-20240813193042254](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813193042254.png)
![image-20240813192641081](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813192641081.png)

cgroup-v2-cpu-idle-001和cgroup-v2-cpu-max-burst-001测试失败的原因是不存在cpu.idle和cpu.burst.max这两个接口文件
cgroup-v2-cpu-pressure测试失败的原因是没有cpu.pressure这个接口文件。

本地qemu：

![image-20240814143203487](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814143203487.png)
![image-20240814143213079](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814143213079.png)

cgroup-v2-cpu-idle-001和cgroup-v2-cpu-max-burst-001测试失败的原因是不存在cpu.idle和cpu.burst.max这两个接口文件
cgroup-v2-cpu-pressure测试失败的原因是没有cpu.pressure这个接口文件。

**OpenCloudOS 9：**

腾讯云：

![image-20240814162257363](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814162257363.png)
![image-20240814162307489](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814162307489.png)

本地qemu：

![image-20240814155032448](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814155032448.png)
![image-20240814155051387](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814155051387.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814184809027](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814184809027.png)
![image-20240814184815898](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814184815898.png)

### memory

memory控制器调节内存的分配。内存是有效状态的，并实现限制和保护模型。

所有内存量均以字节为单位。如果写入的值为与PAGE_SIZE对齐，则读回该值可能会向上舍入为最接近的PAGE_SIZE倍数

#### memory.current

存在于非root cgroup上的只读单值文件

该cgroup及其后代当前正在使用的内存总量

#### memory.min

存在于非root cgroup上的读写单值文件。默认值为0

硬内存保护。如果cgroup的内存使用量在其有效最小边界内，则在任何情况下都不会回收该cgroup的内存。如果没有可用的不受保护的可回收内存，则会调用OOM终止程序。如果超过出有效最小边界(或有效低边界，如果有效低边界更高)，则会按超额比例回收页面，从而减少超额较小的回收压力

如果memory cgroup中没有进程， 则会忽略其memory.min

#### memory.low

存在于非root cgroup上的读写单值文件。默认值为0

尽力保护内存，如果cgroup的内存使用量在其有效低边界内，则除非未受保护的cgroup中没有可回收内存，否则不会回收该cgroup的内存。如果超出有效低边界（或有效最小边界，如果有效最小边界更高），则会根据超额部分按比例回收页面，从而减少超额部分的回收压力

#### memory.high

存在于非root cgroup上的读写单值文件。默认值为max

内存使用限制，如果cgroup的使用量超出上限，则cgroup的进程将受到限制，并承受巨大的回收压力

超过上限不会调用OOM终止程序，在极端情况下可能会突破限制。在外部进程监控受限cgroup以缓解沉重的回收压力的情况下，应使用上限

#### memory.max

存在于非root cgroup上的读写单值文件。默认值为max

内存使用硬限制。这是限制cgroup内存使用的主要机制。如果cgroup的内存使用量达到此限制且无法降低，则会在cgroup中调用OOM killer。在某些情况下， 使用量可能会暂时超过限制

#### memory.peak

存在于非root cgroup上的只读单值文件。自cgroup创建以来，记录的cgroup及其后代的最大内存使用量

#### memory.reclaim

所有cgroup都存在的只写文件

这是一个在目标cgroup中触发内存回收的简单接口

#### memory.oom.group

存在于非root cgroup上的读写单值文件。默认值为0

确定OOM终止程序是否应将cgroup视为不可分割的工作负载。如果设置，则属于该cgroup或其后代（如果内存cgroup不是叶cgroup）的所有任务都会一起终止或根本不终止。这可用于避免部分终止以保证工作负载的完整性

具有OOM保护（oom_score_adj设置为-1000）的任务被视为异常，并且永远不会被终止

如果在cgroup中调用OOM终止程序，它将不会终止该cgroup之外的任何任务，无论父cgroup的memory.oom,group值如何

#### memory.events

存在于非root cgroup上的只读平键文件。定义了以下条目：

- low：尽管cgroup的使用率低于低限，但由于内存压力过大而被回收的次数。
- high：由于超出高内存边界，cgroup的进程被限制以执行直接内存回收的次数。
- max：cgroup的内存使用量即将超过最大边界的次数。如果直接回收无法降低内存使用量，则cgroup会进入oom状态
- oom：cgroup内存使用量达到限制且分配即将失败的次数。如果不将oom killer视为一个选项，则不会引发此事件
- oom_kill：属于此cgroup的被任何类型的OOM终止程序终止的进程数
- oom_group_kill：发生组oom的次数（OpenCloud8没有此字段）

memory.events.local：与memory.events类似，但文件中的字段是cgroup本地的，即不是分层的。在此文件上生成的文件修改仅反映本地事件

#### memory.swap.current

存在于非root cgroup上的只读单值文件

该cgroup及其后代当前正在使用的交换总量

#### memory.swap.high

存在于非root cgroup上的读写单值文件。默认值为max

此限制标志着cgroup的不可逆转点。它并非设置用于管理工作负载在正常运行期间进行的交换量。与memory.swap.max相比，后者进制交换量超过设定值，但只要其他内存可以回收，cgroup就可以不受阻碍地继续运行

#### memory.swap.peak

存在于非root cgroup上的只读单值文件

自cgroup创建以来，记录的cgroup及其后代的最大交换使用情况

#### memory.swap.max

存在于非root cgroup上的读写单值文件，默认值为max

交换使用硬限制。如果cgroup的交换使用量达到此限制，则不会将cgroup的匿名内存换出

#### memory.swap.events

存在于非root cgroup上的只读平键文件。定义了以下条目。除非另有说明，否则此文件中的值更改会生成文件修改事件

- high：cgroup的交换使用量超出高阈值的次数
- max：cgroup的交换使用量即将超出最大边界且交换分配失败的次数
- fail：由于系统范围内的交换空间用尽或最大限制而导致交换分配失败的次数

当在当前使用率下减少时，现有的交换条目将逐渐被回收，并且交换使用率可能在较长时间内保持高于限制，着减少了对工作负载和内存管理的影响

#### memory.zswap.current

存在于非root cgroup上只读单值文件

zswap压缩后端消耗的内存总量

#### memory.zswap.max

存在于非root cgroup上的读写单值文件。默认值为max

zswap使用硬限制。如果cgroup的zswap池达到此限制，它将拒绝在现有条目故障恢复或写入磁盘之前接受任何存储

#### memory.pressure

只读的嵌套键文件。显示内存的压力失速信息。

#### 测试用例

- **cgroup-v2-memory-current：** 测试memory.current接口文件，分配一些匿名内存和页缓存，查看memory.current的值
- **cgroup-v2-memory-min-001：** 测试memoruy.min接口文件，parent1/2的父cgroup设置的内存上限是200MB, parent2分配的内存为146MB, 不会占用parent1的memory.min保护的内存50MB
- **cgroup-v2-memory-min-002：** 测试memoruy.min接口文件，parent1/2的父cgroup设置的内存上限是200MB, parent2分配的内存为170MB, 会占用parent1的memory.min保护的内存50MB，因此会被kill
-  **cgroup-v2-memory-low-001：** 测试memoruy.low接口文件，parent1/2的父cgroup设置的内存上限是200MB, parent2分配的内存为146MB, 不会占用parent1的memory.low保护的内存50MB
-  **cgroup-v2-memory-low-002：** 测试memoruy.low接口文件，parent1/2的父cgroup设置的内存上限是200MB, parent2分配的内存为170MB, 会占用parent1的memory.low保护的内存50MB, 但是不会被kill，依然能分配
- **cgroup-v2-memory-high-001：** 测试memory.high接口文件，测试是否能有效控制匿名内存和页缓存所消耗的内存量
- **cgroup-v2-memory-high-002：** 测试memory.high接口文件，测试是否能有效对内存的大量单次分配进行限制
- **cgroup-v2-memory-max：** 测试memory.max接口文件
- **cgroup-v2-memory-reclaim-001：** 测试memory.reclaim接口文件，回收page cache
- **cgroup-v2-memory-reclaim-002：** 测试memory.reclaim接口文件，回收匿名内存
- **cgroup-v2-memory-oom-group-001：** 测试memory.oom.group接口文件，分配匿名内存达到oom限制，检查子cgroup进程是否被kill
- **cgroup-v2-memory-oom-group-002：** 测试memory.oom.group接口文件，分配匿名内存达到oom限制，检查父cgroup和子cgroup进程是否被kill
- **cgroup-v2-memory-oom-group-003：**  测试memory.oom.group接口文件，分配匿名内存达到oom限制，检查除了设置OOM_SCORE_ADJ_MIN的进程外，其他进程是否被kill
- **cgroup-v2-memory-oom-events：** 测试memory.events中的oom和oom_kill事件
- **cgroup-v2-memory-swap-high：** 测试memory.swap.high和memory.swap.current接口文件
- **cgroup-v2-memory-swap-max：** 测试memory.swap.max接口文件
- **cgroup-v2-memory-zswap：** 测试memory.zswap.max和memory.zswap.current接口文件, 达到限制不会触发写回操作
- **cgroup-v2-memory-pressure：** 测试memory.pressure接口文件，设置内存压力通知

#### 测试结果

**OpenCloudOS 8：**

腾讯云：

![image-20240813215939420](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813215939420.png)

cgroup-v2-memory-pressure测试用例失败的原因是没有cgroup.preesure接口文件

cgroup-v2-memory-reclaim-001/2测试用例失败的原因是没有memory.reclaim接口文件

cgroup-v2-memory-swap-high/max测试用例跳过的原因是没有swapfile

cgroup-v2-memory-zswap测试用例失败的原因是没有memory.zswap.max接口文件

本地qemu：

![image-20240814142044608](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814142044608.png)

cgroup-v2-memory-pressure测试用例失败的原因是没有cgroup.preesure接口文件

cgroup-v2-memory-reclaim-001/2测试用例失败的原因是没有memory.reclaim接口文件

cgroup-v2-memory-swap-high/max测试用例失败的原因是没有memory.swap.high接口文件

**OpenCloudOS 9：**

腾讯云：

![image-20240814141912888](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814141912888.png)

cgroup-v2-memory-reclaim-002测试用例失败的原因是没有不能正常回收匿名内存，当分配匿名内存时，无法写入memory.reclaim接口文件
cgroup-v2-memory-zswap测试用例失败的原因，推测是无法正常使用zswap，尽管设定了zswap.max的值，但是分配的内存还是会被kill掉，无法利用到zswap

本地qemu：

![image-20240814155628208](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814155628208.png)

仍然是cgroup-v2-memory-zswap测试用例测试失败

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814185220725](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814185220725.png)

### IO

io控制器控制io资源的分配，此控制器可实现基于权重和绝对带宽或IOPS限制的分配

#### io.stat

只读的嵌套键文件

行由\$MAJ：\$MIN设备编号键控，且无序。定义了以下嵌套键：

| 字段   | 含义         |
| ------ | ------------ |
| rbytes | 读取的字节数 |
| wbytes | 写入的字节数 |
| rios   | 读取IO数量   |
| wios   | 写IO次数     |
| dbytes | 丢弃的字节数 |
| dios   | 丢弃IO数     |

#### io.weight

存在于非 root cgroup 上的读写平键文件。默认值为“default 100”。

第一行是应用于设备的默认权重，没有特定的覆盖。其余的覆盖由 \$MAJ：\$MIN 设备编号键入且无序。权重在 [1, 10000] 范围内，指定 cgroup 相对于其同级可以使用的相对 IO 时间量。

可以通过写入“default \$WEIGHT”或简单的“​\$WEIGHT”来更新默认权重。可以通过写入“\$MAJ：\$MIN ​\$WEIGHT”来设置覆盖，并通过写入“​\$MAJ：\$MIN default”来取消设置。

注意：需要先在root cgroup启用特定设备的io.cost.qos，才能将设备填充到io.weight，例如：

 ```bash
ehco "8：0 enabled=1 ctrl=auto" > io.cost.qos
 ```

#### io.max

存在于非 root cgroup 上的读写嵌套键文件。

基于 BPS 和 IOPS 的 IO 限制。行由 \$MAJ：\$MIN 设备编号键控且无序。定义了以下嵌套键：

| 字段  | 含义                   |
| ----- | ---------------------- |
| rbps  | 每秒最大读取字节数     |
| wbps  | 每秒最大写入字节数     |
| riops | 每秒最大读取 IO 操作数 |
| wiops | 每秒最大写入 IO 操作数 |

写入时，可以以任意顺序指定任意数量的嵌套键值对。可以指定“max”作为值来删除特定限制。如果多次指定相同的键，则结果不确定。

#### io.pressure

只读的嵌套键文件，显示 IO 的压力失速信息。

#### 测试用例

- **cgroup-v2-io-weight：** 测试io.weight接口文件
- **cgroup-v2-io-max-001：** 测试io.max接口文件的rbps
- **cgroup-v2-io-max-002：** 测试io.max接口文件的wbps
- **cgroup-v2-io-max-003：** 测试io.max接口文件的riops
- **cgroup-v2-io-max-004：** 测试io.max接口文件的wiops、
- **cgroup-v2-io-pressure：** 测试io.pressure接口文件，设置io压力通知

#### 测试结果

**OpenCloudOS 8：**

腾讯云：

![image-20240813220144401](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813220144401.png)

cgroup-v2-io-pressure测试用例失败的原因是没有cgroup.preesure接口文件

本地qemu：

![image-20240814121312911](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814121312911.png)

cgroup-v2-io-pressure测试用例失败的原因是没有cgroup.preesure接口文件

**OpenCloudOS 9：**

腾讯云：

![image-20240814142222974](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814142222974.png)

本地qemu：

![image-20240814155752495](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814155752495.png)

同样是跟上面的cgroup v1中的cgroup-v1-blkio-throttle-write_bps_device和cgroup-v1-blkio-throttle-write_iops_device测试用例一样，不能正常限制写硬盘速率。

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814185236668](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814185236668.png)

同上，不能正常限制写硬盘速率。

### PID

允许cgroup在达到一定限制后阻止任何新任务被fork()或clone()

**接口文件：**

- pids.max：存在于非 root cgroup 上的读写单值文件。默认值为“max”。进程数量的硬性限制。
- pids.current：存在于非 root cgroup 上的只读单值文件。cgroup 及其后代中当前的进程数。
- pids.peak：存在于非 root cgroup 上的只读单值文件。该 cgroup 及其后代进程数曾达到的最大值。
- pids.events：存在于非 root cgroup 上的只读平键文件。除非另有说明，否则此文件中的值更改会生成文件修改事件。定义了以下条目：
  - max：cgroup 的总进程数达到 pids.max 限制的次数
- pids.events.local：与 pids.events 类似，但文件中的字段是 cgroup 本地的，即不是分层的。在此文件上生成的文件修改事件仅反映本地事件。

pids.current > pids.max 是可能的，这可以通过将限制设置为小于 pids.current 或将足够多的进程附加到 cgroup 以使 pids.current 大于 pids.max 来实现。但是，不可能通过 fork() 或 clone() 违反 cgroup PID 策略。

**测试用例：**

- **cgroup-v2-pids-max-001：** 测试pids.max接口文件, 进程数超过限制
- **cgroup-v2-pids-max-002：** 测试pids.max控制文件, 进程数不超过限制
- **cgroup-v2-pids-max-003：** 测试pids的层次结构

**测试结果：**

**OpenCloudOS 8：**

腾讯云：

![image-20240813222209839](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813222209839.png)

本地qemu：

![image-20240814121333373](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814121333373.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814142241006](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814142241006.png)

本地qemu：

![image-20240814155915955](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814155915955.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814185415242](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814185415242.png)

### cpuset

“cpuset”控制器提供了一种机制，用于将任务的 CPU 和内存节点放置限制为仅任务当前 cgroup 中的 cpuset 接口文件中指定的资源。这在大型 NUMA 系统中尤其有用，在这些系统中，将作业放置在适当大小的系统子集上，并仔细放置处理器和内存以减少跨节点内存访问和争用，可以提高整体系统性能。

“cpuset” 控制器是分层的。这意味着控制器不能使用其父级中不允许的 CPU 或内存节点。

#### cpuset.cpus

存在于非 root cpuset 启用的 cgroup 上的读写多值文件。

它列出了此 cgroup 内的任务所请求的 CPU。但实际授予的 CPU 列表受其父级施加的限制，可能与请求的 CPU 不同。

CPU 编号是用逗号分隔的数字或范围，例如：0,3-4

空值表示 cgroup 使用与具有非空“cpuset.cpus”的最近的 cgroup 祖先相同的设置，如果未找到任何可用 CPU，则使用所有可用的 CPU。

“cpuset.cpus” 的值保持不变直到下次更新，不会受到任何 CPU 热插拔事件的影响。

#### cpuset.cpus.effective

所有启用 cpuset 的 cgroup 上都存在的只读多值文件。

它列出了此 cgroup 的父级实际授予其的在线 CPU。这些 CPU 允许由当前 cgroup 中的任务使用。

如果“cpuset.cpus”为空，“cpuset.cpus.effective”文件将显示父 cgroup 中可供此 cgroup 使用的所有 CPU。否则，它应该是“cpuset.cpus”的子集，除非“cpuset.cpus”中列出的 CPU 均无法授予。在这种情况下，它将被视为空的“cpuset.cpus”。

其值会受到CPU热插拔事件的影响。

#### cpuset.cpus.partition

这个文件可以被设置为root或member，主要功能是用来设置当前的cgroup是不是作为一个独立的scheduling domain进行调度。这个功能其实就是可以理解为，在root模式下，所有分配给当前cgroup的cpu都是独占这些cpu的，而member模式则可以在多个cgroup之间共享这些cpu。设置为root将使当前cgroup使用的cpu从上级cgroup的cpuset.cpus.effective列表中拿走，即上一级不能使用这个被独占的CPU了。设置为root后，如果这个cgroup有下一级的cgroup，这个cgroup也将不能再切换回member状态。在这种模式下，上一级的cgroup不可以把自己所有的cpu都分配给其下一级的cgroup，其自身至少给自己留一个cpu

设置为root需要当前cgroup符合以下条件：

- cpuset.cpus中设置不为空且设置的cpu list中的cpu都是独立的。就是说这些cpu不会共享给其他平级cgroup
- 上一级cgroup是partion root配置
- 当前cgroup的cpuset.cpus作为集合是上一级cgroup的cpuset.cpus.effective集合的子集
- 下一级cgroup没有启用cpuset资源隔离

#### cpuset.mems

存在于非 root cpuset 启用的 cgroup 上的读写多值文件。

它列出了此 cgroup 内的任务所请求的内存节点。但实际授予的内存节点列表受其父级施加的限制，可能与请求的内存节点不同。

内存节点号是用逗号分隔的数字或范围。例如：0-1,3

空值表示 cgroup 使用与最近的 cgroup 祖先相同的设置

“cpuset.mems” 的值保持不变直到下次更新，不会受到任何内存节点热插拔事件的影响。

如果“cpuset.mems”当前正在使用指定节点之外的内存，则将 cgroup 内的任务的内存迁移到指定节点。

这种内存迁移是有代价的。迁移可能不完整，可能会遗留一些内存页。因此，建议在将新任务生成到 cpuset 之前正确设置“cpuset.mems”。即使需要使用活动任务更改“cpuset.mems”，也不应该频繁进行。

#### cpuset.mems.effective

所有启用 cpuset 的 cgroup 上都存在的只读多值文件。

它列出了此 cgroup 的父级实际授予其的在线内存节点。这些内存节点允许由当前 cgroup 内的任务使用。

如果“cpuset.mems”为空，则显示父 cgroup 中可供此 cgroup 使用的所有内存节点。否则，它应该是“cpuset.mems”的子集，除非“cpuset.mems”中列出的任何内存节点都无法授予。在这种情况下，它将被视为空的“cpuset.mems”。

其值会受到内存节点热插拔事件的影响。

#### 测试用例

- **cgroup-v2-cpuset-cpus：** 测试cpuset.cpus和cpuset.cpus.effetive接口文件
- **cgroup-v2-cpuset-cpus-partition：** 测试cpuset.cpus.patition接口文件
- **cgroup-v2-cpuset-mems：** 测试cpuset.mems和cpuset.mems.effetive接口文件

#### 测试结果

**OpenCloudOS 8：**

腾讯云：

![image-20240813222250409](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813222250409.png)

cgroup-v2-cpuset-cpus-partition测试用例要在2个以上cpu才能测试，因为root cgroup始终要占有一个cpu

cgroup-v2-cpuset-mems测试用例需要在2个及以上NUMA节点才能测试

本地qemu：

![image-20240814121353069](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814121353069.png)

**OpenCloudOS 9：**

腾讯云：

![image-20240814142314782](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814142314782.png)

cgroup-v2-cpuset-cpus-partition测试用例要在2个以上cpu才能测试，因为root cgroup始终要占有一个cpu

cgroup-v2-cpuset-mems测试用例需要在2个及以上NUMA节点才能测试

本地qemu：

![image-20240814155933002](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814155933002.png)

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814185448434](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814185448434.png)

### hugetlb

HugeTLB 控制器允许限制每个控制组的 HugeTLB 使用情况，并在页面错误期间强制执行控制器限制。

**接口文件：**

- **hugetlb.2MB.max：** 读写文件。设置hugepage大小为2MB的hugepage使用量限制
- **hugetlb.1GB.max：** 读写文件。设置hugepage大小为1GB的hugepage使用量限制
- **hugetlb.2MB.current：** 只读文件。显示hugepages大小为2MB的hugepage的当前使用量
- **hugetlb.1GB.current：** 只读文件。显示hugepages大小为1GB的hugepage的当前使用量
- **hugetlb.2MB.rsvd.max：** 读写文件。设置hugepage大小为2MB的hugepage预留限制
- **hugetlb.1GB.rsvd.max：** 读写文件。设置hugepage大小为1GB的hugepage预留限制
- **hugetlb.2MB.rsvd.current：** 只读文件。显示hugepages大小为2MB的hugepage的当前预留量
- **hugetlb.1GB.rsvd.current：** 只读文件。显示hugepages大小为1GB的hugepage的当前预留量
- **hugetlb..2MB.events：** 存在于非 root cgroup 上的只读平键文件，包含以下字段：
  - max：由于 HugeTLB 限制导致分配失败的次数
- **hugetlb.1GB.events：** 存在于非 root cgroup 上的只读平键文件，包含以下字段：
  - max：由于 HugeTLB 限制导致分配失败的次数

**测试用例：**

- **cgroup-v2-hugetlb-max-001：** 测试hugetlb.2MB.max接口文件，超过限制
- **cgroup-v2-hugetlb-max-002：** 测试hugetlb.2MB.max接口文件， 不超过限制
- **cgroup-v2-hugetlb-max-003：** 测试hugetlb.1GB.max接口文件， 超过限制
- **cgroup-v2-hugetlb-max-004：** 测试hugetlb.1GB.max接口文件， 不超过限制
- **cgroup-v2-hugetlb-rsvd-max-001：** 测试hugetlb.2MB.rsvd.max接口文件，超过限制
- **cgroup-v2-hugetlb-rsvd-max-002：** 测试hugetlb.2MB.rsvd.max接口文件，不超过限制
- **cgroup-v2-hugetlb-rsvd-max-003：** 测试hugetlb.1GB.rsvd.max接口文件， 超过限制
- **cgroup-v2-hugetlb-rsvd-max-004：** 测试hugetlb.1GB.rsvd.max接口文件， 不超过限制

**测试结果： **

**OpenCloudOS 8：**

腾讯云：

![image-20240813222437274](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240813222437274.png)

都跳过测试的原因：不能启用hugetlb控制器，即root cgroup 的cgroup.control没有包含该控制器

本地qemu：

![image-20240814121500396](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814121500396.png)

都跳过测试的原因：不能启用hugetlb控制器，即root cgroup 的cgroup.control没有包含该控制器

**OpenCloudOS 9：**

腾讯云：

![image-20240814142348511](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814142348511.png)

本地qemu：

![image-20240814155949999](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814155949999.png)

修改hugepage大小为1G后测试：

![image-20240814161512867](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814161512867.png)

cgroup-v2-hugetlb-max-003/004测试失败，即hugetlb.1GB.max没有正常发挥效果

**OpenCloudOS Stream 23：**

本地qemu：

![image-20240814185505769](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814185505769.png)

修改hugepage大小为1G后测试：

![image-20240814190419664](%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A.assets/image-20240814190419664.png)

cgroup-v2-hugetlb-max-003/004测试失败，即hugetlb.1GB.max没有正常发挥效果

